{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path0 = 'Rankings2'\n",
    "path1 = 'Dane'\n",
    "path2 = 'duw'\n",
    "\n",
    "path = path1 + '/' + path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FLduwb00002U.rul',\n",
       " 'FLduwb00003U.rul',\n",
       " 'FLduwb00004U.rul',\n",
       " 'FLduwb00005U.rul',\n",
       " 'FLduwb00006U.rul',\n",
       " 'FLduwb00007U.rul',\n",
       " 'FLduwb00008U.rul',\n",
       " 'FLduwb00009U.rul',\n",
       " 'FLduwb00010U.rul',\n",
       " 'MLduwb00002U.rul',\n",
       " 'MLduwb00003U.rul',\n",
       " 'MLduwb00004U.rul',\n",
       " 'MLduwb00005U.rul',\n",
       " 'MLduwb00006U.rul',\n",
       " 'MLduwb00007U.rul',\n",
       " 'MLduwb00008U.rul',\n",
       " 'MLduwb00009U.rul',\n",
       " 'MLduwb00010U.rul']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['FT1duwb00002URses.csv',\n",
       " 'FT1duwb00003URses.csv',\n",
       " 'FT1duwb00004URses.csv',\n",
       " 'FT1duwb00005URses.csv',\n",
       " 'FT1duwb00006URses.csv',\n",
       " 'FT1duwb00007URses.csv',\n",
       " 'FT1duwb00008URses.csv',\n",
       " 'FT1duwb00009URses.csv',\n",
       " 'FT1duwb00010URses.csv',\n",
       " 'FT2duwb00002URses.csv',\n",
       " 'FT2duwb00003URses.csv',\n",
       " 'FT2duwb00004URses.csv',\n",
       " 'FT2duwb00005URses.csv',\n",
       " 'FT2duwb00006URses.csv',\n",
       " 'FT2duwb00007URses.csv',\n",
       " 'FT2duwb00008URses.csv',\n",
       " 'FT2duwb00009URses.csv',\n",
       " 'FT2duwb00010URses.csv',\n",
       " 'MT1duwb00002URses.csv',\n",
       " 'MT1duwb00003URses.csv',\n",
       " 'MT1duwb00004URses.csv',\n",
       " 'MT1duwb00005URses.csv',\n",
       " 'MT1duwb00006URses.csv',\n",
       " 'MT1duwb00007URses.csv',\n",
       " 'MT1duwb00008URses.csv',\n",
       " 'MT1duwb00009URses.csv',\n",
       " 'MT1duwb00010URses.csv',\n",
       " 'MT3duwb00002URses.csv',\n",
       " 'MT3duwb00003URses.csv',\n",
       " 'MT3duwb00004URses.csv',\n",
       " 'MT3duwb00005URses.csv',\n",
       " 'MT3duwb00006URses.csv',\n",
       " 'MT3duwb00007URses.csv',\n",
       " 'MT3duwb00008URses.csv',\n",
       " 'MT3duwb00009URses.csv',\n",
       " 'MT3duwb00010URses.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rul_files = [f for f in os.listdir(path) if f.endswith('.rul')]\n",
    "# rul_files = ['FLduwb00002U.rul']\n",
    "display(rul_files)\n",
    "text_files = [f for f in os.listdir(path) if f.endswith('.csv') and f[1]=='T']\n",
    "# text_files = ['FT1duwb00002URses.csv','FT2duwb00002URses.csv']\n",
    "display(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f[0]=='M dla duw\n",
    "#f[1]=='M dla dsF, dsK, duf\n",
    "M=[f for f in rul_files if f[0 if path[-1]=='w' else 1]=='M']\n",
    "F=[f for f in rul_files if f[0]=='F']\n",
    "\n",
    "MT=[[f for f in text_files if f[0]=='M' and f[-14:-8]==x[-10:-4]] for x in M]\n",
    "FT=[[f for f in text_files if f[0]=='F' and f[-14:-8]==x[-10:-4]] for x in F]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F = ['FLduwb00002U.rul', 'FLduwb00003U.rul', 'FLduwb00004U.rul', 'FLduwb00005U.rul', 'FLduwb00006U.rul', 'FLduwb00007U.rul', 'FLduwb00008U.rul', 'FLduwb00009U.rul', 'FLduwb00010U.rul']\n",
      "FT = [['FT1duwb00002URses.csv', 'FT2duwb00002URses.csv'], ['FT1duwb00003URses.csv', 'FT2duwb00003URses.csv'], ['FT1duwb00004URses.csv', 'FT2duwb00004URses.csv'], ['FT1duwb00005URses.csv', 'FT2duwb00005URses.csv'], ['FT1duwb00006URses.csv', 'FT2duwb00006URses.csv'], ['FT1duwb00007URses.csv', 'FT2duwb00007URses.csv'], ['FT1duwb00008URses.csv', 'FT2duwb00008URses.csv'], ['FT1duwb00009URses.csv', 'FT2duwb00009URses.csv'], ['FT1duwb00010URses.csv', 'FT2duwb00010URses.csv']]\n",
      "M = ['MLduwb00002U.rul', 'MLduwb00003U.rul', 'MLduwb00004U.rul', 'MLduwb00005U.rul', 'MLduwb00006U.rul', 'MLduwb00007U.rul', 'MLduwb00008U.rul', 'MLduwb00009U.rul', 'MLduwb00010U.rul']\n",
      "MT = [['MT1duwb00002URses.csv', 'MT3duwb00002URses.csv'], ['MT1duwb00003URses.csv', 'MT3duwb00003URses.csv'], ['MT1duwb00004URses.csv', 'MT3duwb00004URses.csv'], ['MT1duwb00005URses.csv', 'MT3duwb00005URses.csv'], ['MT1duwb00006URses.csv', 'MT3duwb00006URses.csv'], ['MT1duwb00007URses.csv', 'MT3duwb00007URses.csv'], ['MT1duwb00008URses.csv', 'MT3duwb00008URses.csv'], ['MT1duwb00009URses.csv', 'MT3duwb00009URses.csv'], ['MT1duwb00010URses.csv', 'MT3duwb00010URses.csv']]\n"
     ]
    }
   ],
   "source": [
    "print('F =',F)\n",
    "print('FT =',FT)\n",
    "print('M =',M)\n",
    "print('MT =',MT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_results(testfile: list, classifier: Classifier, revert=False)->pd.DataFrame:\n",
    "    g1=[(y,x) for y in ['length','support'] for x in ['min','avg','max']]\n",
    "    g2=[('number of rules','')]\n",
    "    g3=[('accuracy',x) for x in testfile]\n",
    "    #row_list = classifier.ranking.index if revert else reversed(classifier.ranking.index)\n",
    "    row_list = list(reversed(classifier.ranking.index))\n",
    "    #chodzi o pełen zbiór\n",
    "    row_list.insert(0,'full')\n",
    "    col_list = pd.MultiIndex.from_tuples(g1+g2+g3)\n",
    "    g=pd.DataFrame(np.nan,row_list,col_list)\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #BOT to TOP\n",
    "# def classify(classifier: Classifier, results: pd.DataFrame, test_files: list):\n",
    "#     rules_copy=classifier.rules.copy()\n",
    "#     ranking=reversed(classifier.ranking.index)\n",
    "#     for attr in ranking:\n",
    "#         #length\n",
    "#         length=classifier.rules.rule.apply(lambda x: len(x))\n",
    "#         results.at[attr,('length','min')]=min(length, default=0)\n",
    "#         results.at[attr,('length','avg')]=sum(length)/len(length) if len(length) else 0\n",
    "#         results.at[attr,('length','max')]=max(length, default=0)\n",
    "#         #support\n",
    "#         support=classifier.rules.support\n",
    "#         results.at[attr,('support','min')]=min(support, default=0)\n",
    "#         results.at[attr,('support','avg')]=sum(support)/len(support) if len(support) else 0\n",
    "#         results.at[attr,('support','max')]=max(support, default=0)\n",
    "#         #number of rules \n",
    "#         results.at[attr,('number of rules','')]=len(classifier.rules)\n",
    "        \n",
    "#         for test_file in test_files:\n",
    "#             classifier.loadTestSet(path+'/'+test_file)\n",
    "            \n",
    "#             (cm, acc)=classifier.classify()\n",
    "            \n",
    "#             #accuracy\n",
    "#             results.at[attr,('accuracy',test_file)]=acc\n",
    "        \n",
    "#         #odfiltrowanie ostatniego atrybutu // zostawia tylko te reguły które nie zawierają tego atrybutu\n",
    "#         classifier.rules=classifier.rules.loc[lambda x:pd.isna(x[attr])]\n",
    "    \n",
    "#     classifier.rules=rules_copy\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward\n",
    "def classify(classifier: Classifier, results: pd.DataFrame, test_files: list):\n",
    "    rules_copy=classifier.rules.copy()\n",
    "    attrs_in_rules=[]\n",
    "    for attr in classifier.ranking.index:\n",
    "        #print(attr)\n",
    "        attrs_in_rules.append(attr)\n",
    "        \n",
    "        #wybór reguł zawierających minimum jeden z powyższych atrybutów\n",
    "        condition=pd.Series(False,classifier.rules.index)\n",
    "        for y in attrs_in_rules:\n",
    "            condition=condition|pd.notna(classifier.rules[y]) \n",
    "        classifier.rules=classifier.rules.loc[condition]\n",
    "        \n",
    "        #length\n",
    "        length=classifier.rules.rule.apply(lambda x: len(x))\n",
    "        results.at[attr,('length','min')]=min(length, default=0)\n",
    "        results.at[attr,('length','avg')]=sum(length)/len(length) if len(length) else 0\n",
    "        results.at[attr,('length','max')]=max(length, default=0)\n",
    "        #support\n",
    "        support=classifier.rules.support\n",
    "        results.at[attr,('support','min')]=min(support, default=0)\n",
    "        results.at[attr,('support','avg')]=sum(support)/len(support) if len(support) else 0\n",
    "        results.at[attr,('support','max')]=max(support, default=0)\n",
    "        #number of rules \n",
    "        results.at[attr,('number of rules','')]=len(classifier.rules)\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            classifier.loadTestSet(path+'/'+test_file)\n",
    "            \n",
    "            (cm, acc)=classifier.classify()\n",
    "            #acc=sum([cm[y][y1] for y,y1 in zip(cm.columns,cm.index)])/len(classifier.test_df)\n",
    "            \n",
    "            #accuracy\n",
    "            results.at[attr,('accuracy',test_file)]=acc\n",
    "        \n",
    "        #powrót do pełnego zestawu reguł\n",
    "        classifier.rules=rules_copy\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward\n",
    "def classify2(classifier: Classifier, results: pd.DataFrame, test_files: list):\n",
    "    rules_copy=classifier.rules.copy()\n",
    "    ranking=reversed(classifier.ranking.index)\n",
    "    not_in_ranking = list(set(classifier.rules.columns[:-3]) - set(classifier.ranking.index))\n",
    "    #TODO: DUW3 pełen zbiór\n",
    "    # klasyfikacja dla pełnego zbioru\n",
    "    for attr in ['full']:\n",
    "        #length\n",
    "        length=classifier.rules.rule.apply(lambda x: len(x))\n",
    "        results.at[attr,('length','min')]=min(length, default=0)\n",
    "        results.at[attr,('length','avg')]=sum(length)/len(length) if len(length) else 0\n",
    "        results.at[attr,('length','max')]=max(length, default=0)\n",
    "        #support\n",
    "        support=classifier.rules.support\n",
    "        results.at[attr,('support','min')]=min(support, default=0)\n",
    "        results.at[attr,('support','avg')]=sum(support)/len(support) if len(support) else 0\n",
    "        results.at[attr,('support','max')]=max(support, default=0)\n",
    "        #number of rules \n",
    "        results.at[attr,('number of rules','')]=len(classifier.rules)\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            classifier.loadTestSet(path+'/'+test_file)\n",
    "            \n",
    "            (cm, acc)=classifier.classify()\n",
    "            \n",
    "            #accuracy\n",
    "            results.at[attr,('accuracy',test_file)]=acc\n",
    "            \n",
    "    # odfiltrowanie atrybutów nie występujących w rankingu\n",
    "    for attr in not_in_ranking:\n",
    "        classifier.rules=classifier.rules.loc[lambda x:pd.isna(x[attr])]\n",
    "    \n",
    "    for attr in ranking:\n",
    "        #length\n",
    "        length=classifier.rules.rule.apply(lambda x: len(x))\n",
    "        results.at[attr,('length','min')]=min(length, default=0)\n",
    "        results.at[attr,('length','avg')]=sum(length)/len(length) if len(length) else 0\n",
    "        results.at[attr,('length','max')]=max(length, default=0)\n",
    "        #support\n",
    "        support=classifier.rules.support\n",
    "        results.at[attr,('support','min')]=min(support, default=0)\n",
    "        results.at[attr,('support','avg')]=sum(support)/len(support) if len(support) else 0\n",
    "        results.at[attr,('support','max')]=max(support, default=0)\n",
    "        #number of rules \n",
    "        results.at[attr,('number of rules','')]=len(classifier.rules)\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            classifier.loadTestSet(path+'/'+test_file)\n",
    "            \n",
    "            (cm, acc)=classifier.classify()\n",
    "            \n",
    "            #accuracy\n",
    "            results.at[attr,('accuracy',test_file)]=acc\n",
    "        \n",
    "        #odfiltrowanie ostatniego atrybutu // zostawia tylko te reguły które nie zawierają tego atrybutu\n",
    "        classifier.rules=classifier.rules.loc[lambda x:pd.isna(x[attr])]\n",
    "    \n",
    "    classifier.rules=rules_copy\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRanking(filepath: str):\n",
    "    with open(filepath, 'r') as plik:\n",
    "        lista = [line.strip() for line in plik]\n",
    "\n",
    "    df = pd.DataFrame({'': lista})\n",
    "    df.set_index('', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsFix(results: pd.DataFrame,atrn: int) -> pd.DataFrame:\n",
    "    attrs=list(range(len(results), 0, -1))\n",
    "    attrs[0]=atrn\n",
    "    results.insert(0,'number of attributes',attrs)\n",
    "    results['mean']=results['accuracy'].mean(axis=1)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE TO xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "def df_to_excel(df,filepath,sheet_name):\n",
    "    # Otwórz istniejący plik Excel\n",
    "    workbook = load_workbook(filepath)\n",
    "\n",
    "    # Konwertuj DataFrame na arkusz Excel\n",
    "    writer = pd.ExcelWriter(filepath, engine='openpyxl') \n",
    "    writer.book = workbook\n",
    "\n",
    "    # Dopisz DataFrame do nowego arkusza\n",
    "    df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    # Zapisz zmiany w pliku\n",
    "    writer.save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier()\n",
    "# False = backward \n",
    "# True = forward\n",
    "reverts=[True]\n",
    "#algotithms=['oner','greedy','reducts']\n",
    "algotithms=['greedy']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy  True  MLduwb00002U.rul  is processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy  True  MLduwb00003U.rul  is processing...\n",
      "greedy  True  MLduwb00004U.rul  is processing...\n",
      "greedy  True  MLduwb00005U.rul  is processing...\n",
      "greedy  True  MLduwb00006U.rul  is processing...\n",
      "greedy  True  MLduwb00007U.rul  is processing...\n",
      "greedy  True  MLduwb00008U.rul  is processing...\n",
      "greedy  True  MLduwb00009U.rul  is processing...\n",
      "greedy  True  MLduwb00010U.rul  is processing...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for alg in algotithms:\n",
    "    for revert in reverts:\n",
    "        # MALE\n",
    "        for rulefile, test_files in zip(M,MT):\n",
    "            print(alg+' ',revert,' '+rulefile, ' is processing...')\n",
    "            \n",
    "            #Wczytanie reguł\n",
    "            classifier.loadRuleSet(path+'/'+rulefile)\n",
    "            \n",
    "            #Wczytanie rankingu\n",
    "            if alg=='oner':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingOnerMale.txt')\n",
    "            elif alg=='greedy':\n",
    "                classifier.loadRanking(path0+'/'+path2+'/'+rulefile[:-4]+'Rses.ranking')\n",
    "            elif alg=='reducts':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingReductsMale.txt')\n",
    "            else:\n",
    "                print('sth wrong')\n",
    "                    \n",
    "            #Stworzenie DF wynikowego\n",
    "            results=create_df_results(test_files,classifier,revert)\n",
    "            \n",
    "            #Przeprowadzenie klasyfikacji i obrobienie wyników\n",
    "            results=classify2(classifier,results,test_files) if revert else classify(classifier,results,test_files)\n",
    "            results=resultsFix(results,classifier.ATRN)\n",
    "            \n",
    "            #Zapisy do plików\n",
    "            df_to_excel(results,'Results2/Results_Male.xlsx',sheet_name=path2+rulefile[-6:-5])\n",
    "            \n",
    "        # FEMALE\n",
    "        for rulefile, test_files in zip(F,FT):\n",
    "            print(alg+' ',revert,' '+rulefile, ' is processing...')\n",
    "            \n",
    "            #Wczytanie reguł\n",
    "            classifier.loadRuleSet(path+'/'+rulefile)\n",
    "            \n",
    "            #Wczytanie rankingu\n",
    "            if alg=='oner':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingOnerFemale.txt')\n",
    "            elif alg=='greedy':\n",
    "                classifier.loadRanking(path0+'/'+path2+'/'+rulefile[:-4]+'Rses.ranking')\n",
    "            elif alg=='reducts':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingReductsFemale.txt')\n",
    "            else:\n",
    "                print('sth wrong')\n",
    "            \n",
    "            #Stworzenie DF wynikowego\n",
    "            results=create_df_results(test_files,classifier,revert)\n",
    "            \n",
    "            #Przeprowadzenie klasyfikacji i obrobienie wyników\n",
    "            results=classify2(classifier,results,test_files) if revert else classify(classifier,results,test_files)\n",
    "            results=resultsFix(results,classifier.ATRN)\n",
    "            \n",
    "            #Zapisy do plików\n",
    "            df_to_excel(results,'Results2/Results_Female.xlsx',sheet_name=path2+rulefile[-6:-5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "US",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ee59d526477b7afe0163b6a0d1dc36f4130464713dea179c55705287fd16ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
