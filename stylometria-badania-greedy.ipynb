{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path0 = 'Rankings2'\n",
    "path1 = 'Dane'\n",
    "path2 = 'duw'\n",
    "\n",
    "path = path1 + '/' + path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_files = [f for f in os.listdir(path) if f.endswith('.rul')]\n",
    "# rul_files = ['FLduwb00002U.rul']\n",
    "display(rul_files)\n",
    "text_files = [f for f in os.listdir(path) if f.endswith('.csv') and f[1]=='T']\n",
    "# text_files = ['FT1duwb00002URses.csv','FT2duwb00002URses.csv']\n",
    "display(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f[0]=='M dla duw\n",
    "#f[1]=='M dla dsF, dsK, duf\n",
    "M=[f for f in rul_files if f[0 if path[-1]=='w' else 1]=='M']\n",
    "F=[f for f in rul_files if f[0]=='F']\n",
    "\n",
    "MT=[[f for f in text_files if f[0]=='M' and f[-14:-8]==x[-10:-4]] for x in M]\n",
    "FT=[[f for f in text_files if f[0]=='F' and f[-14:-8]==x[-10:-4]] for x in F]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F =',F)\n",
    "print('FT =',FT)\n",
    "print('M =',M)\n",
    "print('MT =',MT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_results(testfile: list, classifier: Classifier, revert=False)->pd.DataFrame:\n",
    "    g1=[(y,x) for y in ['length','support'] for x in ['min','avg','max']]\n",
    "    g2=[('number of rules','')]\n",
    "    g3=[('accuracy',x) for x in testfile]\n",
    "    #row_list = classifier.ranking.index if revert else reversed(classifier.ranking.index)\n",
    "    row_list = list(reversed(classifier.ranking.index))\n",
    "    #chodzi o pełen zbiór\n",
    "    row_list.insert(0,'full')\n",
    "    col_list = pd.MultiIndex.from_tuples(g1+g2+g3)\n",
    "    g=pd.DataFrame(np.nan,row_list,col_list)\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward\n",
    "def classify(classifier: Classifier, results: pd.DataFrame, test_files: list):\n",
    "    rules_copy=classifier.rules.copy()\n",
    "    attrs_in_rules=[]\n",
    "    for attr in classifier.ranking.index:\n",
    "        #print(attr)\n",
    "        attrs_in_rules.append(attr)\n",
    "        \n",
    "        #wybór reguł zawierających minimum jeden z powyższych atrybutów\n",
    "        condition=pd.Series(False,classifier.rules.index)\n",
    "        for y in attrs_in_rules:\n",
    "            condition=condition|pd.notna(classifier.rules[y]) \n",
    "        classifier.rules=classifier.rules.loc[condition]\n",
    "        \n",
    "        #length\n",
    "        length=classifier.rules.rule.apply(lambda x: len(x))\n",
    "        results.at[attr,('length','min')]=min(length, default=0)\n",
    "        results.at[attr,('length','avg')]=sum(length)/len(length) if len(length) else 0\n",
    "        results.at[attr,('length','max')]=max(length, default=0)\n",
    "        #support\n",
    "        support=classifier.rules.support\n",
    "        results.at[attr,('support','min')]=min(support, default=0)\n",
    "        results.at[attr,('support','avg')]=sum(support)/len(support) if len(support) else 0\n",
    "        results.at[attr,('support','max')]=max(support, default=0)\n",
    "        #number of rules \n",
    "        results.at[attr,('number of rules','')]=len(classifier.rules)\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            classifier.loadTestSet(path+'/'+test_file)\n",
    "            \n",
    "            (cm, acc)=classifier.classify()\n",
    "            #acc=sum([cm[y][y1] for y,y1 in zip(cm.columns,cm.index)])/len(classifier.test_df)\n",
    "            \n",
    "            #accuracy\n",
    "            results.at[attr,('accuracy',test_file)]=acc\n",
    "        \n",
    "        #powrót do pełnego zestawu reguł\n",
    "        classifier.rules=rules_copy\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward\n",
    "def classify2(classifier: Classifier, results: pd.DataFrame, test_files: list):\n",
    "    rules_copy=classifier.rules.copy()\n",
    "    ranking=reversed(classifier.ranking.index)\n",
    "    not_in_ranking = list(set(classifier.rules.columns[:-3]) - set(classifier.ranking.index))\n",
    "    #TODO: DUW3 pełen zbiór\n",
    "    # klasyfikacja dla pełnego zbioru\n",
    "    for attr in ['full']:\n",
    "        #length\n",
    "        length=classifier.rules.rule.apply(lambda x: len(x))\n",
    "        results.at[attr,('length','min')]=min(length, default=0)\n",
    "        results.at[attr,('length','avg')]=sum(length)/len(length) if len(length) else 0\n",
    "        results.at[attr,('length','max')]=max(length, default=0)\n",
    "        #support\n",
    "        support=classifier.rules.support\n",
    "        results.at[attr,('support','min')]=min(support, default=0)\n",
    "        results.at[attr,('support','avg')]=sum(support)/len(support) if len(support) else 0\n",
    "        results.at[attr,('support','max')]=max(support, default=0)\n",
    "        #number of rules \n",
    "        results.at[attr,('number of rules','')]=len(classifier.rules)\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            classifier.loadTestSet(path+'/'+test_file)\n",
    "            \n",
    "            (cm, acc)=classifier.classify()\n",
    "            \n",
    "            #accuracy\n",
    "            results.at[attr,('accuracy',test_file)]=acc\n",
    "            \n",
    "    # odfiltrowanie atrybutów nie występujących w rankingu\n",
    "    for attr in not_in_ranking:\n",
    "        classifier.rules=classifier.rules.loc[lambda x:pd.isna(x[attr])]\n",
    "    \n",
    "    for attr in ranking:\n",
    "        #length\n",
    "        length=classifier.rules.rule.apply(lambda x: len(x))\n",
    "        results.at[attr,('length','min')]=min(length, default=0)\n",
    "        results.at[attr,('length','avg')]=sum(length)/len(length) if len(length) else 0\n",
    "        results.at[attr,('length','max')]=max(length, default=0)\n",
    "        #support\n",
    "        support=classifier.rules.support\n",
    "        results.at[attr,('support','min')]=min(support, default=0)\n",
    "        results.at[attr,('support','avg')]=sum(support)/len(support) if len(support) else 0\n",
    "        results.at[attr,('support','max')]=max(support, default=0)\n",
    "        #number of rules \n",
    "        results.at[attr,('number of rules','')]=len(classifier.rules)\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            classifier.loadTestSet(path+'/'+test_file)\n",
    "            \n",
    "            (cm, acc)=classifier.classify()\n",
    "            \n",
    "            #accuracy\n",
    "            results.at[attr,('accuracy',test_file)]=acc\n",
    "        \n",
    "        #odfiltrowanie ostatniego atrybutu // zostawia tylko te reguły które nie zawierają tego atrybutu\n",
    "        classifier.rules=classifier.rules.loc[lambda x:pd.isna(x[attr])]\n",
    "    \n",
    "    classifier.rules=rules_copy\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRanking(filepath: str):\n",
    "    with open(filepath, 'r') as plik:\n",
    "        lista = [line.strip() for line in plik]\n",
    "\n",
    "    df = pd.DataFrame({'': lista})\n",
    "    df.set_index('', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsFix(results: pd.DataFrame,atrn: int) -> pd.DataFrame:\n",
    "    attrs=list(range(len(results), 0, -1))\n",
    "    attrs[0]=atrn\n",
    "    results.insert(0,'number of attributes',attrs)\n",
    "    results['mean']=results['accuracy'].mean(axis=1)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE TO xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "def df_to_excel(df,filepath,sheet_name):\n",
    "    # Otwórz istniejący plik Excel\n",
    "    workbook = load_workbook(filepath)\n",
    "\n",
    "    # Konwertuj DataFrame na arkusz Excel\n",
    "    writer = pd.ExcelWriter(filepath, engine='openpyxl') \n",
    "    writer.book = workbook\n",
    "\n",
    "    # Dopisz DataFrame do nowego arkusza\n",
    "    df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    # Zapisz zmiany w pliku\n",
    "    writer.save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier()\n",
    "# False = backward \n",
    "# True = forward\n",
    "reverts=[True]\n",
    "#algotithms=['oner','greedy','reducts']\n",
    "algotithms=['greedy']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for alg in algotithms:\n",
    "    for revert in reverts:\n",
    "        # MALE\n",
    "        for rulefile, test_files in zip(M,MT):\n",
    "            print(alg+' ',revert,' '+rulefile, ' is processing...')\n",
    "            \n",
    "            #Wczytanie reguł\n",
    "            classifier.loadRuleSet(path+'/'+rulefile)\n",
    "            \n",
    "            #Wczytanie rankingu\n",
    "            if alg=='oner':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingOnerMale.txt')\n",
    "            elif alg=='greedy':\n",
    "                classifier.loadRanking(path0+'/'+path2+'/'+rulefile[:-4]+'Rses.ranking')\n",
    "            elif alg=='reducts':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingReductsMale.txt')\n",
    "            else:\n",
    "                print('sth wrong')\n",
    "                    \n",
    "            #Stworzenie DF wynikowego\n",
    "            results=create_df_results(test_files,classifier,revert)\n",
    "            \n",
    "            #Przeprowadzenie klasyfikacji i obrobienie wyników\n",
    "            results=classify2(classifier,results,test_files) if revert else classify(classifier,results,test_files)\n",
    "            results=resultsFix(results,classifier.ATRN)\n",
    "            \n",
    "            #Zapisy do plików\n",
    "            df_to_excel(results,'Results2/Results_Male.xlsx',sheet_name=path2+rulefile[-6:-5])\n",
    "            \n",
    "        # FEMALE\n",
    "        for rulefile, test_files in zip(F,FT):\n",
    "            print(alg+' ',revert,' '+rulefile, ' is processing...')\n",
    "            \n",
    "            #Wczytanie reguł\n",
    "            classifier.loadRuleSet(path+'/'+rulefile)\n",
    "            \n",
    "            #Wczytanie rankingu\n",
    "            if alg=='oner':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingOnerFemale.txt')\n",
    "            elif alg=='greedy':\n",
    "                classifier.loadRanking(path0+'/'+path2+'/'+rulefile[:-4]+'Rses.ranking')\n",
    "            elif alg=='reducts':\n",
    "                classifier.ranking=loadRanking('Rankings/RankingReductsFemale.txt')\n",
    "            else:\n",
    "                print('sth wrong')\n",
    "            \n",
    "            #Stworzenie DF wynikowego\n",
    "            results=create_df_results(test_files,classifier,revert)\n",
    "            \n",
    "            #Przeprowadzenie klasyfikacji i obrobienie wyników\n",
    "            results=classify2(classifier,results,test_files) if revert else classify(classifier,results,test_files)\n",
    "            results=resultsFix(results,classifier.ATRN)\n",
    "            \n",
    "            #Zapisy do plików\n",
    "            df_to_excel(results,'Results2/Results_Female.xlsx',sheet_name=path2+rulefile[-6:-5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "US",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ee59d526477b7afe0163b6a0d1dc36f4130464713dea179c55705287fd16ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
